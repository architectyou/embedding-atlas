#!/usr/bin/env python3
"""
Multi-Dataset Embedding Atlas CLI
ê° parquet íŒŒì¼ë³„ë¡œ ë³„ë„ì˜ endpointë¥¼ ì œê³µí•˜ëŠ” embedding atlas ì„œë²„
"""

import logging
import pathlib
import socket
from pathlib import Path
from typing import Dict, List

import click
import pandas as pd
import uvicorn

# FastAPIì™€ ê¸°ë³¸ ë¼ìš°íŒ…
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse, RedirectResponse
from fastapi.staticfiles import StaticFiles

# embedding-atlas backend ëª¨ë“ˆ ì„í¬íŠ¸
from packages.backend.embedding_atlas.data_source import DataSource
from packages.backend.embedding_atlas.server import make_server
from packages.backend.embedding_atlas.utils import Hasher


class MultiDatasetAtlas:
    """ë‹¤ì¤‘ ë°ì´í„°ì…‹ì„ ê´€ë¦¬í•˜ëŠ” Embedding Atlas"""

    def __init__(self, static_path: str, duckdb_uri: str = "wasm"):
        self.datasets: Dict[str, DataSource] = {}
        self.static_path = static_path
        self.duckdb_uri = duckdb_uri
        self.app = FastAPI(title="Multi-Dataset Embedding Atlas")

        # CORS ì„¤ì •
        self.app.add_middleware(
            CORSMiddleware,
            allow_origins=["*"],
            allow_methods=["*"],
            allow_headers=["*"],
            expose_headers=["*"],
        )

        self._setup_routes()

    def add_dataset(self, name: str, parquet_path: str, vector_column: str = "vector"):
        """ë°ì´í„°ì…‹ ì¶”ê°€"""
        print(f"Loading dataset '{name}' from {parquet_path}")

        # Parquet íŒŒì¼ ë¡œë“œ
        df = pd.read_parquet(parquet_path)
        print(f"Loaded {len(df)} rows, {len(df.columns)} columns")

        # ë©”íƒ€ë°ì´í„° ì„¤ì •
        id_column = "_row_index"
        if id_column not in df.columns:
            df[id_column] = range(df.shape[0])

        # í…ìŠ¤íŠ¸ ì»¬ëŸ¼ ì°¾ê¸° (text, content, description ë“±)
        text_column = None
        for col in ["text", "content", "description", "sentence", "document"]:
            if col in df.columns:
                text_column = col
                break

        metadata = {
            "columns": {
                "id": id_column,
                "text": text_column,
            },
            "dataset_name": name,
            "file_path": parquet_path,
            "vector_column": vector_column,
        }

        # ë²¡í„°ê°€ ìˆë‹¤ë©´ ì„ë² ë”© ì •ë³´ ì¶”ê°€
        if vector_column in df.columns:
            # ë²¡í„°ì—ì„œ x, y ì¢Œí‘œ ê³„ì‚° (UMAP ë˜ëŠ” ë¯¸ë¦¬ ê³„ì‚°ëœ ì¢Œí‘œ)
            x_col, y_col = self._compute_or_find_coordinates(df, vector_column)
            if x_col and y_col:
                metadata["columns"]["embedding"] = {
                    "x": x_col,
                    "y": y_col,
                }

        # neighbors ì»¬ëŸ¼ ì°¾ê¸° (__neighbors, neighbors ë“±) - UMAP ê³„ì‚° í›„ì— ì²´í¬
        neighbors_column = None
        for col in ["__neighbors", "neighbors", "neighbor_ids"]:
            if col in df.columns:
                neighbors_column = col
                break

        # neighbors ì»¬ëŸ¼ì´ ìˆë‹¤ë©´ ë©”íƒ€ë°ì´í„°ì— ì¶”ê°€
        if neighbors_column:
            metadata["columns"]["neighbors"] = neighbors_column
            print(f"Found neighbors column: {neighbors_column}")

        # ë°ì´í„°ì†ŒìŠ¤ ìƒì„±
        hasher = Hasher()
        hasher.update([parquet_path])
        hasher.update(metadata)
        identifier = hasher.hexdigest()

        dataset = DataSource(identifier, df, metadata)
        self.datasets[name] = dataset

        print(f"Dataset '{name}' added successfully")
        return dataset

    def _compute_or_find_coordinates(self, df, vector_column):
        """ì¢Œí‘œ ê³„ì‚° ë˜ëŠ” ê¸°ì¡´ ì¢Œí‘œ ì°¾ê¸°"""
        # ì´ë¯¸ x, y ì¢Œí‘œê°€ ìˆëŠ”ì§€ í™•ì¸
        x_candidates = ["x", "projection_x", "umap_x", "tsne_x", "embedding_x"]
        y_candidates = ["y", "projection_y", "umap_y", "tsne_y", "embedding_y"]

        x_col = None
        y_col = None

        for col in x_candidates:
            if col in df.columns:
                x_col = col
                break

        for col in y_candidates:
            if col in df.columns:
                y_col = col
                break

        if x_col and y_col:
            return x_col, y_col

        # ì¢Œí‘œê°€ ì—†ìœ¼ë©´ ë²¡í„°ë¡œë¶€í„° UMAP ê³„ì‚°
        if vector_column in df.columns:
            print(f"Computing UMAP projection for {vector_column}...")
            try:
                import numpy as np
                import umap
                from sklearn.neighbors import NearestNeighbors

                # ë²¡í„° ì¶”ì¶œ
                vectors = []
                for idx, row in df.iterrows():
                    vector = row[vector_column]
                    if isinstance(vector, (list, np.ndarray)):
                        vectors.append(np.array(vector))
                    else:
                        vectors.append(vector)

                vectors = np.array(vectors)

                # UMAP ì ìš©
                reducer = umap.UMAP(
                    n_components=2,
                    n_neighbors=15,
                    min_dist=0.1,
                    metric="cosine",
                    random_state=42,
                )

                projection = reducer.fit_transform(vectors)

                # DataFrameì— ì¶”ê°€
                x_col = "projection_x"
                y_col = "projection_y"
                df[x_col] = projection[:, 0]
                df[y_col] = projection[:, 1]

                # Nearest neighbors ê³„ì‚° (ìœ ì‚¬ ì„ ìˆ˜ ê¸°ëŠ¥ì„ ìœ„í•´)
                print("Computing nearest neighbors for similarity search...")
                nn = NearestNeighbors(n_neighbors=21, metric='cosine')  # 20ê°œ + ìê¸° ìì‹ 
                nn.fit(vectors)

                # ê° ì ì— ëŒ€í•œ nearest neighbors ì €ì¥
                neighbors_list = []
                for i in range(len(vectors)):
                    distances, indices = nn.kneighbors([vectors[i]])
                    # ìê¸° ìì‹  ì œì™¸í•˜ê³  ìƒìœ„ 20ê°œ
                    neighbor_indices = [int(idx) for idx in indices[0][1:21]]
                    neighbor_distances = [float(dist) for dist in distances[0][1:21]]
                    # ê¸°ë³¸ CLIì™€ ê°™ì€ í˜•ì‹ìœ¼ë¡œ ì €ì¥: {"distances": [...], "ids": [...]}
                    neighbors_list.append({
                        "distances": neighbor_distances,
                        "ids": neighbor_indices
                    })

                # DataFrameì— neighbors ì¶”ê°€
                df["__neighbors"] = neighbors_list

                print(f"UMAP projection completed: {x_col}, {y_col}")
                print("Nearest neighbors computed for similarity search")
                return x_col, y_col

            except ImportError:
                print("UMAP or sklearn not available, skipping projection")
                return None, None
            except Exception as e:
                print(f"Error computing projection: {e}")
                return None, None

        return None, None

    def _setup_routes(self):
        """API ë¼ìš°íŠ¸ ì„¤ì •"""

        @self.app.get("/")
        async def root():
            """ë£¨íŠ¸: ì‚¬ìš© ê°€ëŠ¥í•œ ë°ì´í„°ì…‹ ëª©ë¡"""
            return {
                "message": "Multi-Dataset Embedding Atlas",
                "datasets": list(self.datasets.keys()),
                "endpoints": [f"/dataset/{name}" for name in self.datasets.keys()],
            }

        @self.app.get("/datasets")
        async def list_datasets():
            """ë°ì´í„°ì…‹ ëª©ë¡ê³¼ ë©”íƒ€ë°ì´í„°"""
            result = {}
            for name, dataset in self.datasets.items():
                metadata = dataset.metadata.copy()
                metadata.update(
                    {
                        "num_rows": len(dataset.dataset),
                        "num_columns": len(dataset.dataset.columns),
                        "columns": list(dataset.dataset.columns),
                    }
                )
                result[name] = metadata
            return result

        @self.app.get("/dataset/{dataset_name}")
        async def redirect_to_dataset(dataset_name: str):
            """íŠ¹ì • ë°ì´í„°ì…‹ìœ¼ë¡œ ë¦¬ë‹¤ì´ë ‰íŠ¸"""
            if dataset_name not in self.datasets:
                raise HTTPException(
                    status_code=404, detail=f"Dataset '{dataset_name}' not found"
                )

            # ì •ì  íŒŒì¼ ì„œë¹™ì„ ìœ„í•´ dataset-specific ê²½ë¡œë¡œ ë¦¬ë‹¤ì´ë ‰íŠ¸
            return RedirectResponse(url=f"/dataset/{dataset_name}/")

        @self.app.get("/dataset/{dataset_name}/info")
        async def get_dataset_info(dataset_name: str):
            """ë°ì´í„°ì…‹ ì •ë³´"""
            if dataset_name not in self.datasets:
                raise HTTPException(
                    status_code=404, detail=f"Dataset '{dataset_name}' not found"
                )

            dataset = self.datasets[dataset_name]
            metadata = dataset.metadata.copy()
            metadata.update(
                {
                    "num_rows": len(dataset.dataset),
                    "num_columns": len(dataset.dataset.columns),
                    "columns": list(dataset.dataset.columns),
                }
            )
            return metadata

    def mount_dataset_routes(self):
        """ê° ë°ì´í„°ì…‹ë³„ë¡œ ê²½ë¡œ ë§ˆìš´íŠ¸"""
        for name, dataset in self.datasets.items():
            # ê° ë°ì´í„°ì…‹ìš© ê°œë³„ FastAPI ì•± ìƒì„±
            dataset_app = make_server(
                dataset, static_path=self.static_path, duckdb_uri=self.duckdb_uri
            )

            # ì„œë¸Œ ì•±ìœ¼ë¡œ ë§ˆìš´íŠ¸
            self.app.mount(f"/dataset/{name}", dataset_app)
            print(f"Mounted dataset '{name}' at /dataset/{name}")


def find_available_port(start_port: int, max_attempts: int = 10, host="localhost"):
    """ì‚¬ìš© ê°€ëŠ¥í•œ í¬íŠ¸ ì°¾ê¸°"""
    for port in range(start_port, start_port + max_attempts):
        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
            if s.connect_ex((host, port)) != 0:
                return port
    raise RuntimeError("No available ports found in the given range")


@click.command()
@click.option("--host", default="localhost", help="Host address")
@click.option("--port", default=5055, help="Port number")
@click.option(
    "--auto-port/--no-auto-port", default=True, help="Auto find available port"
)
@click.option("--duckdb", default="wasm", help="DuckDB connection mode")
@click.option("--static", default=None, help="Custom static files path")
def main(host, port, auto_port, duckdb, static):
    """Multi-Dataset Embedding Atlas CLI"""

    logging.basicConfig(level=logging.INFO, format="%(levelname)s: %(message)s")

    # ì •ì  íŒŒì¼ ê²½ë¡œ ì„¤ì •
    if static is None:
        backend_path = (
            Path(__file__).parent
            / "packages"
            / "backend"
            / "embedding_atlas"
            / "static"
        )
        static = str(backend_path.resolve())

    print(f"Using static files from: {static}")

    # Multi-Atlas ì„œë²„ ì´ˆê¸°í™”
    atlas = MultiDatasetAtlas(static_path=static, duckdb_uri=duckdb)

    parquet_data_dir = Path(__file__).parent / "parquet_data"

    # Parquet íŒŒì¼ë“¤ ìë™ ê°ì§€ ë° ì¶”ê°€
    # ì—”ë“œí¬ì¸íŠ¸ ì´ë¦„, íŒŒì¼ëª…, ë²¡í„° ì»¬ëŸ¼ëª…
    parquet_files = [
        ("scouting_report_bgem3", "scouting_report_bgem3_with_year.parquet", "vector"),
        (
            "scouting_report_openai",
            "scouting_report_openai_with_year.parquet",
            "vector",
        ),
        (
            "scouting_report_qwen8b",
            "scouting_report_qwen8b_with_year.parquet",
            "vector",
        ),
    ]

    for name, file_name, vector_col in parquet_files:
        file_path = parquet_data_dir / file_name
        if file_path.exists():
            try:
                atlas.add_dataset(name, str(file_path), vector_col)
            except Exception as e:
                print(f"Failed to load {file_path}: {e}")
        else:
            print(f"File not found: {file_path}")

    if not atlas.datasets:
        print("No datasets loaded! Please check your parquet files.")
        return

    # ë°ì´í„°ì…‹ë³„ ë¼ìš°íŠ¸ ë§ˆìš´íŠ¸
    atlas.mount_dataset_routes()

    # í¬íŠ¸ ì„¤ì •
    if auto_port:
        final_port = find_available_port(port, max_attempts=10, host=host)
        if final_port != port:
            logging.info(f"Port {port} not available, using {final_port}")
    else:
        final_port = port

    print(f"\nğŸ—ºï¸  Multi-Dataset Embedding Atlas")
    print(f"ğŸ“Š Loaded {len(atlas.datasets)} datasets:")
    for name in atlas.datasets.keys():
        print(f"   â€¢ {name}: http://{host}:{final_port}/dataset/{name}")
    print(f"ğŸ“‹ All datasets: http://{host}:{final_port}/datasets")
    print(f"ğŸŒ Server starting at http://{host}:{final_port}")

    # ì„œë²„ ì‹¤í–‰
    uvicorn.run(atlas.app, host=host, port=final_port, access_log=False)


if __name__ == "__main__":
    main()
